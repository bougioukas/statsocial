[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic Statistics for the Social Sciences",
    "section": "",
    "text": "Welcome!\nThis open course introduces students to key concepts and statistical methods used in the quantitative social and behavioral sciences to describe and test hypotheses about the social world and human behavior. Students will learn to:\n\nDescribe and summarize data distributions.\nFormulate and test various types of research hypotheses.\nAnalyze associations between factors, characteristics, or events.\nInterpret and critically evaluate published statistics.\n\nThe student may also learn about the proper use of graphing and statistical software.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why learn basic statistics?\nKnowledge of the fundamental principles of statistics, data analysis, and research methodology is essential for professionals in psychology and other social sciences today for three main reasons:",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-learn-basic-statistics",
    "href": "intro.html#why-learn-basic-statistics",
    "title": "1  Introduction",
    "section": "",
    "text": "Understanding key statistical indicators: This knowledge allows professionals to interpret data and indicators from sources such as ELSTAT, EUROSTAT, and WHO. With this expertise, they can effectively assess and verify relevant information from around the world, hereby helping to counter misinformation. Key examples of these indicators include population health status, social inequalities, rates of violence and crime, unemployment rates, educational attainment levels, poverty rates, and access to healthcare services.\nCritically analyzing research studies: Professionals are better equipped to evaluate research studies and their findings, ensuring they can identify strengths, weaknesses, and potential biases.\nConducting independent research: Familiarity with statistics enables professionals who conduct their own research to collaborate more effectively with statistical experts, enhancing the overall quality of their work.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-discipline-of-statistics",
    "href": "intro.html#the-discipline-of-statistics",
    "title": "1  Introduction",
    "section": "1.2 The discipline of Statistics",
    "text": "1.2 The discipline of Statistics\nThe word “statistics” originates from the Latin “status”. Initially, it referred to the political state of a region, with “statista” used for recording information like censuses or data on a state’s wealth. Over time, the meaning and use of statistics broadened, and its scope evolved.\n\nToday, Statistics is an applied mathematical science that, according to Croxton and Cowden, can be defined as “the science of collection, presentation, analysis, and interpretation of numerical data”.\n\nStatistics includes different theoretical frameworks such as traditional (frequentist) statistics and Bayesian statistics. In this course, we will cover classical parametric and nonparametric statistical tests of traditional statistics.\nRelying heavily on probability theory and empirical methods, statistics aims to describe and summarize data as well as to draw inferences about the population from which the data are derived. Therefore, the discipline of traditional (frequentist) statistics includes two main branches (Figure 1.1):\n\nDescriptive statistics that includes measures of frequency and measures of location and dispersion. It also includes a description of the shape of the data distributions.\nInferential statistics that aims at generalizing conclusions made on a sample to a whole population. It includes estimation and hypothesis testing.\n\n\n\n\n\n\n\nflowchart LR\n  \n    A[Traditional &lt;br/&gt; Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: &lt;br/&gt; e.g., frequency, percentage.]\n    B --- E[Measures of location &lt;br/&gt; and dispersion: &lt;br/&gt; e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n\n\n\n\nFigure 1.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics\n\n\n\n\n\nIn a research study, both descriptive and inferential statistics are commonly used. First, researchers present descriptive statistics (e.g., demographic data, baseline characteristics) to provide a clear snapshot of the sample. Then, inferential statistics are applied to test hypotheses and draw conclusions about the broader population from which the sample was drawn.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#talking-about-data",
    "href": "intro.html#talking-about-data",
    "title": "1  Introduction",
    "section": "1.3 Talking about data",
    "text": "1.3 Talking about data\n\n1.3.1 The “Age of data”\n\n\n\n\n\nWe are living in the “Age of Data”, where every day, an astonishing 2.5 quintillion (\\(10^{18}\\)) bytes of data are generated worldwide. An example of how this era is transforming scientific research can be seen in large-scale multi-ancestry genome-wide association studies (GWAS), which are used to uncover the genetic basis of complex conditions like anxiety disorders. In a recent study, researchers analyzed genomic data from more than 1.2 million individuals across diverse populations, identifying more than 100 genes associated with stress and anxiety (Friligkou et al. 2024).\nA second example is chatbots, which are designed to simulate conversations with users. ELIZA (1966) was one such program. Its famous “DOCTOR” script emulated a psychotherapist by rephrasing user statements as questions (Figure 1.2). Modern chatbots, such as ChatGPT, Gemini and Copilot, are large language models trained on vast amounts of text data (e.g., books, articles, websites, and user-generated content), hence the term “large”. Using deep learning techniques, these models aim to understand and generate human-like text, effectively responding to a wide range of user queries.\n\n\n\n\n\n\nFigure 1.2: The most famous scenario simulated a psychotherapist of the Rogerian school.\n\n\n\n \nNext is a YouTube video that explores the history of Eliza and chatbots.\n\n\n\n1.3.2 Structure of data\nThere are three main data structures:\n\nStructured data generally refer to highly organized tabular data, facilitating straightforward search, analysis, and processing. Examples include data stored in spreadsheets, such as Excel files, or in comma-separated values (CSV) files.\nSemi-Structured data are a form of structured data that do not follow a strict tabular format but still have some organizational properties. For example, emails are semi-structured; they include fields like sender, recipient, subject, date, and time, and are also organized into folders such as Inbox, Sent, and Trash.\nUnstructured data refer to information that lacks a predefined format or organization, such as open text (e.g., social media posts), images, videos, and other forms of multimedia.\n\nIn this course, we use data organized in a structured format, such as spreadsheets. Tabular data refer to data organized in a table with rows and columns (Figure 1.3). Each row represents an observation (or record), corresponding to a statistical unit in the dataset. The columns represent the variables (or characteristics) of interest. Cells are the individual units where rows and columns intersect. Each cell contains a data value corresponding to the observation for that row and the variable for that column.\n\n\n\n\n\n\nFigure 1.3: A typical excel spreadsheet with row and columns.\n\n\n\n\n\n1.3.3 Sources of social and health data\nData in the social and health sciences refers to the information collected and analyzed to understand human behavior, societal structures and interactions. This data originates from various sources, each offering unique insights into different aspects of human and societal dynamics. These sources include:\n\nSelf-Reports:  These are collected through interviews, questionnaires, and surveys, capturing individual experiences, behaviors, and attitudes. They provide qualitative insights that enhance our understanding of personal perspectives and social phenomena.\nInternet and Social Media:  These platforms generate vast amounts of data on online interactions, behaviors, and social trends, offering valuable information on how people communicate and engage in the digital space.\nWearable Technology:  Devices such as smartwatches, fitness trackers, smart glasses, and smart clothing equipped with sensors have become revolutionary tools for tracking and monitoring physiological and behavioral metrics in real-time. They provide critical insights into health, fitness, and daily habits.\nElectronic Health Records (EHRs):  EHRs offer detailed information about patients’ medical histories, treatments, and health outcomes, facilitating research on health trends and the effectiveness of interventions.\nHealth Surveillance Systems:  These systems continuously monitor and analyze trends in real-time, including disease outbreaks, vaccination uptake, public health patterns, substance abuse trends, and crime statistics, thereby informing timely interventions and policy decisions.\nClinical Registries:  These collect data on patients with specific medical conditions or treatments, providing valuable insights into health outcomes and social determinants of health.\nBiobanks:  Biobanks store biological samples (e.g., blood, tissue) along with health and lifestyle data, enabling research into the intersections of genetics, environment, and social factors.\n\n\n\n1.3.4 From data to knowledge and decisions\nSocial, behavioral, and biomedical data can be transformed into information. This information can evolve into knowledge when social scientists and stakeholders interpret and understand it, allowing them to make informed decisions, shape policies, and implement interventions that address societal and health-related challenges more effectively (Figure 1.4).\n\n\n\n\n\n\nFigure 1.4: From data to knowledge and action.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#social-statistics",
    "href": "intro.html#social-statistics",
    "title": "1  Introduction",
    "section": "1.4 Social Statistics",
    "text": "1.4 Social Statistics\nSocial statistics is a field of statistics applied in the social sciences to study social phenomena and trends. It employs various statistical methods of data collection, such as censuses, social surveys, and administrative records. These methods are commonly used by international organizations, government agencies, institutions, and researchers to analyze data related to social life, human behavior, and society as a whole.\nThe primary goal of social statistics is to provide objective, quantitative evidence that aids in understanding and interpreting social issues, ultimately informing the development of appropriate social policies.\nLet’s explore some examples of official statistics from national governments and international organizations.\nExample 1\n\n\n\n\n\nThrough its quarterly report Greece in Figures, the Hellenic Statistical Authority (ELSTAT) presents current and detailed statistical insights into Greece’s population, social structure, and economy.\nAn important socio-economic indicator at country level is the percentage of people at risk of poverty or social exclusion. Poverty and social exclusion severely disrupt the lives of those affected, perpetuating a cycle of disadvantage that can last for generations. This cycle not only limits personal growth but also poses significant challenges to social cohesion and economic stability within communities.\n\n\n\n\n\n\nFigure 1.5: The percentage of people at risk of poverty or social exclusion in Eurozone. Greece (EL) is highlihgted.\n\n\n\nIn 2023, over a quarter (26.1%) of the Greek population was at risk of poverty or social exclusion, ranking Greece (EL) among the highest in the Eurozone, just behind Spain (ES; 26.5%), as shown in Figure 1.5.\n \nExample 2\n\n\n\n\n\nGender inequalities and violence against women and girls are urgent social and public health issues that require immediate attention. A report, based on data from 2018 and published in May 2021, conducted by the World Health Organization, the London School of Hygiene and Tropical Medicine and the South African Medical Research Council, presents findings on the prevalence of violence against women.\nThe report found that the global lifetime prevalence of intimate partner violence among ever-partnered women was 30.0% (95% CI = 27.8% to 32.2%).\n\nIt is important to note that this estimated percentage (30%) is accompanied by a confidence interval (CI), which indicates a range from 27.8% to 32.2%. This interval reflects the precision of the estimate, and we will discuss it further in the subsequent sections of the course.\n\nThe report also mentioned that the prevalence of intimate partner violence among ever-partnered women was highest in the WHO African, Eastern Mediterranean and South-East Asia Regions with an estimated prevalence of approximately 37%. In contrast, the prevalence was lower in the high-income regions of America (23%) and the European and Western Pacific Regions (25%). This indicates that the prevalence of violence against women can differ significantly among regions, influenced by a range of socio-economic factors. In statistical terms, this suggests that there is a large variance in the data.\n\n\n\n\n\nThe report also references the Kiribati case; a country consisting of 33 islands situated in the central Pacific Ocean. The Kiribati family health study revealed an alarming prevalence of violence against women in Kiribati, with 68% of women aged 15–49 who had been in a relationship reporting experiences of violence (emotional, physical, and/or sexual) from a partner. This finding highlights how social statistics can assist in identifying rare cases (extreme values) to raise global concern and prompt immediate action.\nThe following YouTube video from UN Women is a campaign aimed at raising awareness about violence against women and girls.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#variables",
    "href": "intro.html#variables",
    "title": "1  Introduction",
    "section": "1.5 Variables",
    "text": "1.5 Variables\n\n1.5.1 Independent and dependent variables\nA variable is a quantity or characteristic that is free to vary, or take on different values. Researchers design studies to test whether changes to one or more variables are associated with changes in another variable of interest.\n\n\n\n\n\nFor example, if researchers hypothesize that a psychological intervention can help prevent falls in older adults living in the community more effectively than a standard approach, they could create a study to test this hypothesis. Participants would be randomly assigned to one of two groups: the experimental group, which receives the psychological intervention aimed at preventing falls, and the control group, which receives the usual care.\nIn this example, the type of intervention each participant received (i.e., the psychological intervention vs. usual care) is the independent variable (X), as it is the variable that the researchers manipulate. The dependent variable (Y), or the outcome variable, is the rate of falls over a time period, as it reflects the effect or outcome that the researchers measure to determine whether the intervention has an impact on reducing falls among older adults living in the community.\n\nAn independent variable (X) is the variable that is changed or controlled in a research study to examine its effect on another variable (Y), which represents the outcome being measured.\nA dependent variable (Y) is the variable that is measured and assessed in a research study, influenced by the independent variables (Xs) being studied. It represents the outcome of the study.\nIt is important to note that we can construct both simple bivariate models and more complex, realistic multivariable models:\n\nDiagram of a Bivariate Model (X → Y)\n\nIn this model, X is assumed to influence or predict Y. It’s commonly used to explore simple cause-and-effect associations or correlations, such as how a psychological intervention (X) might affect rate of falls in elderly (Y).\n\n\n\n\n\nflowchart LR\n  A(Independent variable X \\ne.g. psychological intervention) -.-&gt; B(Dependent variable Y \\ne.g. Rate of falls)\n\n\n\n\n\n\n\nDiagram of Multivariable Model (X1, X2, X3 → Y)\n\nThis model allows for a more complex analysis, where several factors are considered simultaneously. For example, it can be used to study how a psychological intervention, gender, and balance (X1, X2, X3) together might affect rate of falls in elderly (Y), helping to capture the combined effects of multiple influences on the outcome.\n\n\n\n\n\nflowchart LR\n  A(Independent variable X1 \\ne.g. psychological intervention) -.-&gt; B(Dependent variable Y \\ne.g. Rate of falls)\n  C(Independent variable X2 \\ne.g. gender) -.-&gt; B\n  D(Independent variable X3 \\ne.g. balance) -.-&gt; B\n\n\n\n\n\n\n\n \n\n\n1.5.2 Confounding variable\nIt is often essential to analyze the association between an independent variable (X) and a dependent variable (Y) while considering confounding variables, which should be controlled to prevent distortion of results.\n\nA confounding variable is defined as one that is related with both the independent and the dependent variables and does not lie on the causal pathway between them.\n\nFor example, consider a study investigating the link between caffeine intake (X) and lung cancer (Y). Without accounting for confounding variables, such as smoking, the results may suggest a misleading association between caffeine and occurrence of lung cancer that is not truly causal.\n\n\n\n\n\n\nFigure 1.6: From data to knowledge and action.\n\n\n\nSmoking may be a potential confounding variable because it is:\n\nrelated to caffeine consumption: Smokers often consume more caffeine (e.g., through coffee or energy drinks).\ndirect Cause of lung cancer: Smoking is a well-established cause of lung cancer and significantly increases the risk of developing the disease.\nnot directly part of the causal pathway: Smoking does not lie in the causal pathway between caffeine and lung cancer.\n\n\n\n\n\n\n\n Comment\n\n\n\nThe causal pathway refers to the sequence of events or mechanisms through which an independent variable influences a dependent variable. It demonstrates how changes in the independent variable lead to changes in the dependent variable, often involving intermediary variables.\n\n\n\n\n\nflowchart LR\n  A(Independent variable X \\ne.g. exercise) -.-&gt; B(Intermediate variable W \\ne.g. release of endorphins)-.-&gt;C(Dependent variable Y \\ne.g. improved mood)\n\n\n\n\n\n\n\n\nConfounding can be controlled either before or after a study is conducted. Various methods are available to control for confounding, including matching, stratification, and more advanced multivariate techniques (Grimes and Schulz 2002).\n\nPairwise matching: In a case-control study where smoking is considered a confounding variable, cases and controls can be matched based on smoking status.\nStratification: After a study is conducted, results can be stratified by levels of the confounding variable. In the smoking example, results would be calculated separately for smokers and non-smokers to determine if the same effect occurs independently of smoking.\nMultivariate techniques: Mathematical modelling examines the potential effect of one variable while simultaneously controlling for the effect of many other variables.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#types-of-data-in-variables",
    "href": "intro.html#types-of-data-in-variables",
    "title": "1  Introduction",
    "section": "1.6 Types of data in variables",
    "text": "1.6 Types of data in variables\nData in variables can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 1.7):\n\n\n\n\n\n\nflowchart TB\n    A[Data in variables]---&gt; B[Categorical data]\n    A[Data in variables]---&gt; C[Numerical data]\n    B ---&gt; E[Nominal&lt;br&gt;e.g. eye color \\nbrown/blue/green etc.]\n    B ---&gt; F[Ordinal&lt;br&gt;e.g. degree of pain&lt;br&gt;minimal/moderate/&lt;br&gt;severe/unbearable, \\nlikert scale]\n    C ---&gt; G[Discrete&lt;br&gt;e.g. number of children]\n    C ---&gt; H[Numerical&lt;br&gt;e.g. height, reaction time]\n    \n   style E color:#980000, stroke:#333,stroke-width:4px\n   style F color:#980000, stroke:#333,stroke-width:4px\n   style G color:#980000, stroke:#333,stroke-width:4px\n   style H color:#980000, stroke:#333,stroke-width:4px\n\n\n\n\nFigure 1.7: Broad classification of the different types of data with examples.\n\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe type of data in variables is an important factor in determining the most appropriate statistical analysis of the data.\n\n\n\n1.6.1 Categorical data\nA. Nominal data\nNominal data consists of distinct, unordered categories that are labeled but not measured, only counted. These categories can be binary, such as diagnosed/not diagnosed with depression, or they can have more than two categories, such as eye color (e.g., brown, blue, green, gray) or type of therapy (e.g., cognitive-behavioral therapy, psychoanalysis, humanistic therapy).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can represent diagnosed/not diagnosed with depression as 1/0 and cognitive-behavioral therapy/psychoanalysis/humanistic therapy as 1/2/3 for therapy type. Unlike numerical data, the numbers assigned to categories do not have mathematical meaning; they are merely codes.\n\n\n \nB. Ordinal data\nWhen categories can be ordered, the data are of ordinal type. For example, patients may rate their pain as minimal, moderate, severe, or unbearable. In this case, there is a natural order to the values, as moderate pain is more intense than minimal but less than severe. Another common example of ordinal data is the Likert scale, where respondents might indicate their level of agreement with a statement on a scale such as from 1 (strongly disagree) to 5 (strongly agree).\n\n\n\n\n\n\nIMPORTANT\n\n\n\n\nOrdinal data are often transformed into binary data to simplify analysis, presentation, and interpretation, though this can result in a loss of information.\nAlthough the Likert scale consists of ordered categories, the levels can be treated like numeric data, allowing scores from a questionnaire to be summed or averaged for a comprehensive assessment of respondents’ attitudes or feelings (total score of the participant).\n\n\n\n\n\n1.6.2 Numerical data\nA. Discrete data\nDiscrete data can take only a finite number of values (usually integers) in a range, for example, the number of children in a family or the number of days missed from work. Other examples are often counts per unit of time such as the number of visits to the psychotherapist in a year, or the number of epileptic seizures a patient has per month.\nIn practice discrete data are often treated in statistical analyses as if they were ordinal data. Although this approach may be acceptable, it may not fully optimize the use of our data.\nB. Continuous data\nContinuous data are numbers (usually with units) that can theoretically take any value within a given range. Examples of continuous variables include height, weight, body temperature, and reaction time. However, in practice, these variables are often measured discretely, constrained by the precision of measuring instruments and the study’s specific objectives. For instance, although height can be a continuous value, it is typically recorded in discrete steps. A person may measure 172.345 cm tall, but this would usually be recorded as 172 cm.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nIt is important to note that continuous data are often categorized to create categorical variables. For example, body mass index (BMI)—a continuous variable that measures weight relative to height—is typically converted into an ordinal variable with four categories: underweight, normal weight, overweight, and obese. However, dividing continuous variables into categories results in a considerable loss of information. Furthermore, the reverse transformation is impossible; a categorical variable cannot be transformed into a continuous variable.\n\n\nNext is a YouTube video by Associate Professor Mike Marin from the University of British Columbia that comprehensively explains the different types of variables.\n\n\n\n\n\nFriligkou, Eleni, Solveig Løkhammer, Brenda Cabrera-Mendoza, Jie Shen, Jun He, Giovanni Deiana, Mihaela Diana Zanoaga, et al. 2024. “Gene Discovery and Biological Insights into Anxiety Disorders from a Large-Scale Multi-Ancestry Genome-Wide Association Study.” Nature Genetics, September. https://doi.org/10.1038/s41588-024-01908-2.\n\n\nGrimes, David A, and Kenneth F Schulz. 2002. “Bias and Causal Associations in Observational Research.” The Lancet 359 (9302): 248–52. https://doi.org/10.1016/s0140-6736(02)07451-2.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "descriptive.html",
    "href": "descriptive.html",
    "title": "2  Descriptive statistics",
    "section": "",
    "text": "2.1 Data\nWe will explore a dataset containing 258 participants (rows) and 8 variables (columns). The variables include sex (female/male), age (in years), time spent on the internet and social media (in hours), the total score (0-30) from responses to 10 questions on the Rosenberg Self-Esteem Scale (RSES), and the categorization of that score into three levels of self-esteem: low (0-15), medium (16-19), and high (20-30) (García et al. 2019).",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "2  Descriptive statistics",
    "section": "2.2 Summarizing categorical data (Frequency Statistics)",
    "text": "2.2 Summarizing categorical data (Frequency Statistics)\n\n2.2.1 One variable frequency tables and plots\nThe first step in analyzing a categorical variable is to count the occurrences of each label and calculate their frequencies. This collection of frequencies for all possible categories is known as the frequency distribution of the variable. Additionally, we can express these frequencies as proportions of the total number of observations, which are referred to as relative frequencies. If we multiply these proportions by 100, we obtain percentages (%).\n \nSex variable\nLet’s create a a frequency table for the sex variable:\n\n\n\n\n\n\nFigure 2.1: Fequency table for the sex.\n\n\n\nThe table displays the following:\n\nAbsolute frequency (n): The number of participants in each category (male: 109, female: 149).\nPercentage (%): The proportion of participants in each category relative to the total number of participants (relative frequency) multiplied by 100% (male: 109/258 x 100 = 42.2%, female: 149/258 x 100 = 57.8%). Note that the percentages sum up to 100% (42.2% + 57.8%).\nCumulative percentage (%): The sum of the percentage contributions of all categories up to and including the current one. For example, for the male category, the cumulative percentage is 42.2%. When combining male and female categories, the cumulative percentage is 42.2% + 57.8% = 100%. Therefore, the final cumulative percentage must equal 100%.\n\nWhile frequency tables are extremely useful, plotting the data often provides a clearer presentation. For categorical variables, such as sex, it is straightforward to display the number of occurrences in each category using bar plots. The x-axis typically represents the categories of the variable—in this case, “male” and “female”. The y-axis represents the frequency or count of occurrences for each category.\n\n\n\n\n\n\n\n\nFigure 2.2: Bar plot showing the frequency distribution of the sex.\n\n\n\n\n\nIf the y-axis represents percentages (%), then each bar’s height corresponds to the percentage of participants in that category. For example, the percentage of female participants is 57.8%.\n\n\n\n\n\n\n\n\nFigure 2.3: Bar plot showing the percentage of participants for each level of self-esteem.\n\n\n\n\n\n \nScore_cat variable (self-esteem)\nSimilarly, we can create the frequency table for the Score_cat (self-esteem) variable:\n\n\n\n\n\n\nFigure 2.4: Frequency table for the levels of self-esteem.\n\n\n\nIn the above table, we observe that 40.3% (104 out of 258) of participants have a low level of self-esteem. When we combine the low and medium categories, the cumulative percentage is: 40.3% + 32.6% = 72.9%. Finally, for all categories (low, medium, high), the cumulative percentage sums to 72.9% + 27.1% = 100%.\nFigure 2.5 illustrates the frequency distribution of self-esteem. The horizontal axis (x-axis) displays the different self-esteem categories, ordered according to increasing self-esteem levels, while the vertical axis (y-axis) shows the frequency of each category.\n\n\n\n\n\n\n\n\nFigure 2.5: Bar plot showing the frequency for each level of self-esteem.\n\n\n\n\n\nFigure 2.6 illustrates the distribution of self-esteem using percentages. Most participants fall into the category of low self-esteem, accounting for 40.3% (172 out of 428), highlighting a significant portion of the sample that may benefit from targeted interventions or support.\n\n\n\n\n\n\n\n\nFigure 2.6: Bar plot showing the percentage of participants for each level of self-esteem.\n\n\n\n\n\n \n\n\n\n\n\n\nTips for simple bar plots\n\n\n\n\nAll bars should have equal width and equal spacing between them.\nThe height of each bar should correspond to the data it represents.\nThe bars should be plotted against a common zero-valued baseline.\n\n\n\n\n\n2.2.2 Two variable tables (Contingency tables) and plots\nA. Frequency contingency table\nIn addition to tabulating each variable separately, we might also be interested in exploring the association between two categorical variables. In this case, the resulting frequency table is a cross-tabulation, where each combination of levels from both variables is displayed. This type of table is called a contingency table because it shows the frequency of each category in one variable (e.g., sex), contingent upon the specific levels of the other variable (e.g., self-esteem), as shown in Figure 2.7.\n\n\n\n\n\n\nFigure 2.7: Frequency contingency table which cross tabulates sex and self-esteem levels. Each cell value in the table represents the count corresponding to the combination of categories from the two variables.\n\n\n\nNote: The table also typically includes row and column totals, also known as marginal totals, that sum the counts for each row and column, respectively.\n \nB. Joint distribution contingency table\nA joint distribution contingency table displays both the frequency of observations across categories of two variables and the percentage distributions of those frequencies. The percentages are calculated by dividing the frequency in each cell by the overall total (258), then multiplying the result by 100. This shows the percentage of the total observations that fall into each category combination.\n\n\n\n\n\n\nFigure 2.8: Joint distribution contingency table. The percentages are calculated by dividing the frequency in each cell by the overall total.\n\n\n\n \nC. Conditional distribution contingency table\nSuppose we are interested in the distribution of self-esteem levels within each sex group, meaning we are observing how self-esteem vary among males and among females. By conditioning on sex, we divide each cell’s frequency by the corresponding row total (row marginal total), rather than the overall total. This method allows us to examine the conditional distribution of self-esteem within each sex group. For example, the percentage of participants with low self-esteem, given that the participant is female, is calculated as (78/149) x 100 ≈ 52.4%.\n\n\n\n\n\n\nFigure 2.9: Conditional distribution contingency table. The percentages are calculated by dividing the frequency in each cell by the row marginal total.\n\n\n\nThis data analysis indicates notable differences in self-esteem levels between male and female participants. Specifically, the percentage of female participants with low self-esteem (52.4%) is substantially greater than that of male participants (23.9%).\n\n\n\n\n\n\n Comment\n\n\n\nIf we are interested in the distribution of sex within each self-esteem level, focusing on how the proportion of males and females varies within each self-esteem category, we condition on self-esteem. This means we divide each cell’s frequency by the corresponding column total. For example, among those with low self-esteem, the percentage of females is (78/104) x 100 ≈ 75.0%\n\n\n \nWe can also graphically present the data in the table shown in Figure 2.9. A side-by-side bar plot (Figure 2.10) or, preferably, a grouped bar plot (Figure 2.11) can facilitate easier visual comparisons.\n\n\n\n\n\n\n\n\nFigure 2.10: Side-by-side bar plot showing by self-esteem level and sex.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.11: Grouped bar plot showing by self-esteem level and sex.\n\n\n\n\n\nAlternatively, we can create a stacked bar plot, where the bars are segmented by self-esteem levels. Figure 2.12 illustrates a 100% stacked bar plot, which displays the percentage of each self-esteem level (low, medium, high) among male and female participants, emphasizing the relative differences within each group. For example, the plot shows that a higher proportion of females have low self-esteem (52.4%) compared to males (23.9%), while males have a higher proportion of medium (43.1%) and high self-esteem (33%) compared to females.\n\n\n\n\n\n\n\n\nFigure 2.12: A horizontal stacked bar plot showing the distribution of self-esteem stratified by sex.\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nOne consideration when using stacked bar plots is the number of variable levels: with many categories, stacked bar plots can become confusing.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data-summary-statistics",
    "href": "descriptive.html#summarizing-numerical-data-summary-statistics",
    "title": "2  Descriptive statistics",
    "section": "2.3 Summarizing numerical data (Summary Statistics)",
    "text": "2.3 Summarizing numerical data (Summary Statistics)\nSummary measures are single numerical values that summarize a set of data. Numeric data can be described using two main types of summary measures (@tbl-measures).\n\nMeasures of central location: These describe the “center” of the data distribution. Common examples include the mean, median, and mode.\nMeasures of dispersion: These quantify the spread of values around the central value. Examples include the range, interquartile range (IQR), variance, and standard deviation.\n\n \n\n\n\nTable 2.1: Common summary measures of central location and dispersion\n\n\n\n\n\n\n\n\n\nMeasures of Central Location\nMeasures of Dispersion\n\n\n\n\n• Mean\n• Variance\n\n\n• Median\n• Standard Deviation\n\n\n• Mode\n• Range (Minimum, Maximum)\n\n\n\n• Interquartile Range (1st and 3rd Quartiles)\n\n\n\n\n\n\n \nAdditionally, measures of shape such as the sample coefficients of skewness and kurtosis provide further insights by revealing the overall shape and characteristics of the distribution.\n\n2.3.1 Measures of central location\nA. Sample Mean or Average\n\nThe arithmetic mean, or average, denoted as \\(\\bar{x}\\), is calculated by dividing the sum of all values in a set by the total number of values in the set.\n\n\\[\\bar{x}= \\frac{Sum \\ of \\ values}{Number \\ of \\ values} \\tag{2.1}\\]\nExample\nWe subset the data to a smaller sample to make it easier to manually calculate the summary measures. As an example, we will use the number of hours per day spent on the internet by 15-year-old females with low self-esteem, which are as follows:\n\\[ 5 \\ \\ \\ \\ \\  4  \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\  8 \\ \\ \\ \\ \\  4 \\]\nTo calculate the mean, we use the Equation 2.1:\n\\[\\bar{x} = \\frac{5 + 4 + 12 + 8 + 4}{5} = \\frac{33}{5} = 6.6 \\ hours\\]\n \nTo highlight how outliers affect the mean, suppose we add a value of 24 to the dataset. This new data point is considered an outlier, as it is substantially higher than the other values in the dataset. Since 24 hours is the maximum possible in a single day, this extreme value clearly stands out from the rest of the observations.\nOur new dataset becomes:\n\\[ 5 \\ \\ \\ \\ \\  4  \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\  8 \\ \\ \\ \\ \\  4 \\ \\ \\ \\ \\  \\underline{24}\\]\nWe can determine the new mean, \\(\\bar{x}_{new}\\), by adding up all six values and dividing by six:\n\\[\\bar{x}_{new}= \\frac{5 + 4 + 12 + 8 + 4 + 24}{6} = \\frac{57}{6} = 9.5 \\ hours\\]\nAfter adding this outlier, the mean increased from 6.6 to 9.5. This significant rise of 2.9 hours illustrates how outliers can distort the average, making it less representative of the dataset.\n \n\nAdvantages of mean\n\nIt uses all the data values in the calculation and is the balance point of the data.\nIt is algebraically defined and thus mathematically manageable.\n\nDisadvantages of mean\n\nIt is highly influenced by the presence of outliers—values that are abnormally high or low—making it a non-resistant summary measure.\nIt cannot be easily determined by simply inspecting the data and is usually not equal to any of the individual values in the sample.\n\n\n \nB. Median of the sample\nThe sample median, denoted as md, is an alternative measure of location that is less sensitive to outliers than mean.\n\nThe median is calculated by first sorting the observed values (i.e. arranging them in an ascending or descending order) and selecting the middle one. If the number of observations is odd, the median corresponds to the number in the middle of the sorted values. If the number of observations is even, the median is the average of the two middle numbers.\n\nExample\nFirst, we sort the observed values from smallest to largest:\nObserved values: \\(\\ \\ \\ \\ \\ 5 \\ \\ \\ \\ \\  4  \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\  8 \\ \\ \\ \\ \\  4\\)\nSorted values: \\(\\ \\ \\ \\ \\ \\ \\ \\ \\  4 \\ \\ \\ \\ \\  4 \\ \\ \\ \\ \\ \\  5 \\ \\ \\ \\ \\ \\  8  \\ \\ \\ \\ \\ 12\\)\nThe number of observations is 5, which is an odd number; therefore, the median corresponds to the value in the middle of the sorted data:\n\\[ 4 \\ \\ \\ \\ \\ 4 \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{5}} \\ \\ \\ \\ \\ 8 \\ \\ \\ \\ \\ 12 \\]\n\\[md = 5 \\ hours\\]  \nNow let’s examine how the median responds to an outlier by adding the value of 24 to the data. In this case, the number of observations becomes 6, which is an even number. The new median, \\(md_{new}\\), is the average of the two middle numbers, 5 and 8:\n\\[ 4 \\ \\ \\ \\ \\ 4 \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{5}} \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{8}} \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\ \\underline{24}\\]\n\\[md_{new} = \\frac{5 + 8}{2} = \\frac{13}{2} = 6.5 \\ hours\\]\nWe observe that the median is not strongly influenced by the addition of the outlier, as it only increased from 5 to 6.5. This demonstrates that the median is more resistant to outliers compared to the mean.\n \n\nAdvantages of median\n\nIt is resistant to extreme values (outliers) compared to the mean.\n\nDisadvantages of median\n\nIt ignores the actual values of the data points, potentially losing some information about the data.\n\n\n \nC. Mode of the sample\nAnother measure of location is the mode of the sample.\n\nMode represents the value that occurs most frequently in a set of data values.\n\nExample\nIn our example, the value of 4 appears twice in the data:\n\\[ 5 \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{4}} \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\ 8 \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{4}} \\]\nTherefore, the mode is:\n\\[ Mode = 4\\]\nIt’s important to note that some datasets may not have a mode if each value occurs only once. For example, if we replace one of the fours with a three:\n\\[ 5 \\ \\ \\ \\ \\  \\underline{3}  \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\  8 \\ \\ \\ \\ \\  4 \\] In this case, no value repeats, so the dataset has no mode.\n \nHowever, if we replace the 12 with an 8, the dataset becomes:\n\\[ 5 \\ \\ \\ \\ \\  \\textcolor{red}{\\textbf{4}}  \\ \\ \\ \\ \\ \\underline{\\textcolor{green}{\\textbf{8}}} \\ \\ \\ \\ \\  \\textcolor{green}{\\textbf{8}} \\ \\ \\ \\ \\  \\textcolor{red}{\\textbf{4}} \\]\nHere, both 4 and 8 appear twice, making the dataset bimodal, meaning it has two modes.\n\\[ \\textcolor{red}{Mode1 = 4}\\] and\n\\[ \\textcolor{green}{Mode2 = 8}\\]\n \n\n\n\n\n\n\n Comment\n\n\n\nWhile the mode is less commonly used for numerical data, it can be a useful measure of central tendency for categorical data, representing the category with the highest frequency.\n\n\n \n\n\n2.3.2 Measures of dispersion\nA. Range of the sample\n\nThe range is the difference between the maximum and minimum values in a dataset.\n\n\\[range = Max - Min \\tag{2.2}\\]\nThe minimum (Min) value represents the lowest value observed in a dataset, while the maximum (Max) value represents the highest value. These values provide valuable insights into the range and potential outliers within the dataset.\nExample\nLet’s determine the range for the sorted data in our example:\n\\[ \\textcolor{black}{\\textbf{4}} \\ \\ \\ \\ \\  4 \\ \\ \\ \\ \\ 5  \\ \\ \\ \\ \\ 8 \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{12}}\\]\nThe minimum is Min = 4 hours, and the maximum is Max = 12 hours. Therefore, according to Equation 2.2:\n\\[ range = 12 - 4 = 8 \\ hours\\]\n \nLet’s add the extreme value of 24 to the data.\n\\[ \\textcolor{black}{\\textbf{4}} \\ \\ \\ \\ \\ 4 \\ \\ \\ \\ \\ 5  \\ \\ \\ \\ \\ 8 \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\ \\underline{\\textcolor{black}{\\textbf{24}}}\\]\nIn this case, the range becomes:\n\\[range = 24 -4 = 20 \\ hours\\]\n\nThe main disadvantages of the range as a measure of dispersion are its sensitivity to outliers and the fact that it uses only the extreme values, ignoring all other data points.\n\n \nB. Inter-quartile range of the sample\nIn the presence of outliers, the interquartile range (IQR) can provide a more accurate measure of the spread of the majority of the data. Before we define the interquartile range (IQR), let’s first clarify some basic concepts, specifically quantiles and quartiles.\nA quantile indicates the value below which a certain proportion of the data falls. The most commonly used quantiles are known as quartiles:\n\n\\(Q_1\\) (lower quartile) represents the value at which 25% of the data falls below it and 75% falls above it.\n\\(Q_2\\) (median) is the middle value when the data is arranged in ascending or descending order.\n\\(Q_3\\) (upper quartile) represents the value at which 75% of the data falls below it and 25% falls above it.\n\n\nInterquartile range is the difference between the third quartile (or upper quartile) and the first quartile (or lower quartile) in an ordered data set.\n\n\\[IQR = Q_3 - Q_1 \\tag{2.3}\\]\nTherefore, the IQR focuses on the middle 50% of the dataset.\n\n\n\n\n\n\nFigure 2.13: Quartiles and inter-quartile range.\n\n\n\nIt’s important to note that different statistical software packages may produce slightly different quartiles and interquartile ranges (IQRs) for the same dataset, especially when there are only a few values present. This discrepancy is due to the numerous definitions of sample quantiles used in statistical software packages (Hyndman and Fan 1996). While discussing these differences is beyond the scope of this introductory course, we will focus on the results provided by Jamovi.\nExample\nIn our example, the first quartile is \\(Q_1 = 4\\) hours, and the third quartile is \\(Q_3 = 8\\) hours.\n\\[ 4 \\ \\ \\ \\ \\  \\textcolor{black}{\\textbf{4}} \\ \\ \\ \\ \\ 5  \\ \\ \\ \\ \\ \\textcolor{black}{\\textbf{8}} \\ \\ \\ \\ \\ 12\\]\nTherefore, the inter-quartile range is:\n\\[IQR = Q_3 - Q1 = 8 - 4 = 4\\]\n \nAfter adding an extreme value such as 24, our example sorted dataset is as follows:\n\\[ 4 \\ \\ \\ \\ \\ 4 \\ \\ \\ \\ \\ 5  \\ \\ \\ \\ \\ 8 \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\ \\underline{24}\\]\nWe would expect \\(Q_1\\) to have a value between 4 and 5, and \\(Q_3\\) to fall between 8 and 12. JAMOVI provides \\(Q_1 = 4.25\\) hours, and \\(Q_3 =11\\) hours. Therefore, the new inter-quartile range is:\n\\[IQR_{new} = Q_3 - Q1 = 11 - 4.25 = 6.75\\]\n\nAs with the range, greater variability in the data typically leads to a larger IQR. However, unlike the range, the IQR is resistant to outliers, as it is not influenced by observations below the first quartile or above the third quartile.\n\n \nC. Sample variance\nSample variance, denoted as \\(s^2\\), is a measure of spread of the data based on the deviations of the data values from the mean. However, when we average these deviations, the sum always equals zero. This occurs because the mean acts as a balance point where the total positive and negative deviations cancel each other out. To resolve this issue, we calculate the variance using squared deviations, which ensures that all values contribute positively to the measure of spread.\n\nMathematically, the sample variance, \\(s^2\\), is calculated as the sum of the squared deviations from the sample mean, divided by the number of observations minus 1.\n\n\\[s^2 = \\frac{\\text{Sum of squared deviations}}{\\text{Number of values} - 1} \\tag{2.4}\\]\nExample\nThe original values are:\n\\[ 5 \\ \\ \\ \\ \\  4  \\ \\ \\ \\ \\ 12 \\ \\ \\ \\ \\  8 \\ \\ \\ \\ \\  4 \\]\nand we have calculated the mean, \\(\\bar{x} = 6.6\\). The deviation (difference) from the mean for the first value is calculated as 5 - 6.6 = -1.6, and for the second value, it is 4 - 6.6 = -2.6, and so on.\nThus, according to Equation 2.4:\n\\[\n\\begin{align*}\ns^2 &= \\frac{(5 - 6.6)^2 + (4 - 6.6)^2 + (12 - 6.6)^2 + (8 - 6.6)^2 + (4 - 6.6)^2}{5 - 1} \\\\\n    &= \\frac{(-1.6)^2 + (-2.6)^2 + (5.4)^2 + (1.4)^2 + (-2.6)^2}{4} \\\\\n    &= \\frac{2.56 + 6.76 + 29.16 + 1.96 + 6.76}{4} \\\\\n    &= \\frac{47.20}{4} \\\\\n    &= 11.80 \\ hours^2\n\\end{align*}\n\\]\n \nNow let’s examine how the variance responds to an outlier by adding the value of 24 to the data. The new mean is \\(\\bar{x}_{new} = 9.5\\), and the variance is calculated as follows:\n\\[\n\\begin{align*}\ns^2 &= \\frac{(5 - 9.5)^2 + (4 - 9.5)^2 + (12 - 9.5)^2 + (8 - 9.5)^2 + (4 - 9.5)^2 + (24 - 9.5)^2}{6 - 1} \\\\\n    &= \\frac{(-4.5)^2 + (-5.5)^2 + (2.5)^2 + (-1.5)^2 + (-5.5)^2 + (14.5)^2}{5} \\\\\n    &= \\frac{20.25 + 30.25 + 6.25 + 2.25 + 30.25 + 210.25}{5} \\\\\n    &= \\frac{299.5}{5} \\\\\n    &= 59.9 \\ hours^2\n\\end{align*}\n\\]\nThe variance is sensitive to outliers because it is based on the squared deviations from the mean. As a result, even a single extreme value can significantly increase the variance, making it a less reliable measure of spread when outliers are present.\n\nThe variance, being expressed in square units, is not the preferred metric for describing the variability of data.\n\n \nD. Standard deviation of the sample\nStandard deviation is one of the most common measures of spread and is particularly useful for assessing how far the data points are distributed from the mean.\n\nStandard deviation, denoted as s or sd, represents the typical distance of observations from the mean. It is calculated as the square root of the sample variance.\n\n\\[s = \\sqrt{s^2} \\tag{2.5}\\]\nExample\nThe standard deviation is:\n\\[s = \\sqrt{11.8} = 3.44 \\ hours\\]\nand is expressed in the same units as the original data values.\n \nWith the addition of the value 25, the standard deviation becomes:\n\\[s = \\sqrt{59.9} = 7.74 \\ hours\\]\n\nThe standard deviation uses all observations in a dataset for its calculation and is expressed in the same units as the original data. However, it is sensitive to outliers, which can substantially influence its value.\n\n \n\n\n2.3.3 Plots for continuous variables\nWhen visualizing continuous data, several types of plots can be used to understand the distribution, spread, and overall patterns in the data.\nIn the following examples, we will use the complete dataset consisting of 258 observations.\n \nA. Frequency histogram\nThe most common way to present the frequency distribution of numerical data, especially when there are many observations, is through a histogram. Histograms visualize the data distribution as a series of bars without gaps between them (unless a particular bin has zero frequency), in contrast to bar plots. Each bar typically represents a range of numeric values known as a bin (or class), with the height of the bar indicating the frequency of observations (counts) within that particular bin. Below are the frequency histograms for age and time_spent:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram of age.\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram of time spent.\n\n\n\n\n\n\n\nFigure 2.14: Histograms of age and time spent variables.\n\n\n\nIn Figure 2.14(a), the age distribution exhibits a symmetrical bell-shaped form, with the highest frequency occurring around 18 years old. The participants’ ages range approximately from 14 to 22 years. In Figure 2.14(b), the time_spent variable follows a right-skewed distribution, with a higher frequency occurring around 3 hours and a range roughly from 0 to 18 hours.\n\n\n\n\n\n\n Comment\n\n\n\nThe visual appearance of a histogram is greatly influenced by the choice of the binwidth (the difference between the lower and upper limits of the bin). If the binwidth is too small (resulting in a large number of bins), the histogram may appear overly detailed and noisy, making it difficult to discern meaningful patterns. Conversely, if the binwidth is too large (resulting in a small number of bins), important features of the data distribution may be obscured. Experimenting with different binwidths helps us find the optimal setting that best represents the data, resulting in a more informative visualization.\n\n\n \nTo summarize, a histogram provides information on:\n\nThe distribution of the data, whether it’s symmetrical or asymmetrical, and the presence of any outliers.\nThe location of the peak(s) in the distribution.\nThe degree of variability within the data, indicating the spread and range covered by the data.\n\n \nB. Density plot\nA density plot is another way to represent the distribution of numerical data, often seen as a smoother version of a histogram (Figure 2.15). Moreover, density curves are typically scaled so that the area under the curve equals one.\n \n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram of age.\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram of time spent.\n\n\n\n\n\n\n\nFigure 2.15: Density plots of age and time spent variables.\n\n\n\n \nC. Box Plots\nBox plots are useful for visualizing the central tendency and spread of continuous data, particularly when comparing distributions across multiple groups.\n\n\n\n\n\n\nFigure 2.16: Anatomy of a (horizontal) boxplot.\n\n\n\nThis type of graph uses boxes and lines to represent the distributions. In Figure 2.16 the box boundaries indicate the interquartile range (IQR), covering the middle 50% of the data, with a horizontal line inside the box representing the median. Whiskers extend from the box to capture the range of the remaining data, providing additional insight into the spread. Data points lying outside the whiskers are displayed as individual dots and are considered potential outliers.\n \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Vertical box plot of age.\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Vertical box plot of time spent.\n\n\n\n\n\n\n\nFigure 2.17: Box plots of age and time spent variables.\n\n\n\n\n\n\n\n\n\n Comment\n\n\n\nWe have already discussed outliers—observations that exhibit unusually large or small values compared to the rest of the dataset. Outliers can arise from several sources: they may result from incorrect measurements, such as data entry errors or instrument malfunctions, or they may originate from a different population than the rest of the data, indicating a “rare” event. Understanding the causes of outliers is essential for proper data analysis and interpretation.\nWe typically identify outliers using the interquartile range (IQR), where any value outside the interval (Q1 - 1.5 * IQR, Q3 + 1.5 * IQR) is considered a potential outlier (Tukey’s method). This interval extends 1.5 times the IQR below the first quartile (Q1) and above the third quartile (Q3).\n\n\n\n\n\n\nGarcía, Jorge Acosta, Francisco Checa y Olmos, Manuel Lucas Matheu, and Tesifón Parrón Carreño. 2019. “Self Esteem Levels Vs Global Scores on the Rosenberg Self-Esteem Scale.” Heliyon 5 (3): e01378. https://doi.org/10.1016/j.heliyon.2019.e01378.\n\n\nHyndman, Rob J., and Yanan Fan. 1996. “Sample Quantiles in Statistical Packages.” The American Statistician 50 (4): 361–65. https://doi.org/10.1080/00031305.1996.10473566.",
    "crumbs": [
      "Statistical Thinking",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "",
    "text": "3.1 Why Jamovi?\nJamovi is a new fee open “3rd generation” statistical software that is built on top of the programming language R (Figure 3.1). Designed from the ground up to be easy to use, Jamovi is a compelling alternative to costly statistical products such as SPSS and SAS.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#why-jamovi",
    "href": "lab1.html#why-jamovi",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "",
    "text": "Figure 3.1: Jamovi is free and open statistical software\n\n\n\n\n\n\n\n\n\nSome other advantages are:\n\n\n\n\nPoint-and-click environment\nIt provides informative tables and neat visuals\nEnables integration with R\nGives access to a user guide and community resources from the Jamovi website",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#downloading-and-installing-jamovi",
    "href": "lab1.html#downloading-and-installing-jamovi",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "3.2 Downloading and installing Jamovi",
    "text": "3.2 Downloading and installing Jamovi\nJamovi is available for Windows (64-bit), macOS, Linux and ChromeOS. Installation on desktop is quite straight-forward. Just go to the Jamovi download page https://www.jamovi.org/download.html, and download the latest version (current release) for your operating system.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#navigating-jamovi",
    "href": "lab1.html#navigating-jamovi",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "3.3 Navigating Jamovi",
    "text": "3.3 Navigating Jamovi\nWhen jamovi first opens, we will see a screen something like in Figure 3.2.\n\n\n\n\n\n\n\n\nFigure 3.2: Jamovi starts up!.\n\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions, and this can be dragged to the left or the right to change their sizes.\n \nLet’s take a quick look at the Jamovi Main Menu, referred to hereafter as the Menu, as shown in Figure 3.3. This Menu is displayed at the very top of the Jamovi screen:\n\n\n\n\n\n\n\n\nFigure 3.3: The menu bar provides access to all functions of the program.\n\n\n\n\n\nThere are six tabs in the Menu (from left to right): File (a layer with three horizontal levels \\(\\equiv\\)), Variables, Data, Analyses, Edit, and Settings (the three dots \\(\\vdots\\) at the top right of the window) tabs. A toolbar appears whenever we click on a Menu tab (Table 3.1).\n \n\n\n\nTable 3.1: Menu and toolbars of Jamovi\n\n\n\n\n\n\n\n\n\nMenu tab\nToolbar\n\n\n\n\n\nFile tab (\\(\\equiv\\))\nThe file tab  allows us to open/import existing files, save and export our files.\n\n\n\n\n\nVariables tab\nThis allows us to view and search our variables in a list view.\n\n\nThis view allows us to easily navigate our variables and do the following:\n\nSearch for a variable by scrolling through the list or search for one by name.\nEdit the variable names and descriptions by double-clicking in the relevant field.\nEdit our variable details by double-clicking on the data symbol (the screen will appear for us to add all the necessary information).\nCreate a new variable by clicking on the in the bottom right corner. |\n\n\n\n\nData tab\nHere we will see our raw data which are organised like Excel in rows and columns. We can also manipulate our data and add new variables when necessary.\n\n\nSpecifically, this tab allows us to do the following:\n\nRename and add details to existing variables. Click on the Setup button, or double-click on the variable we want to manage.\nCompute and transform variables\nAdd and/or Delete variables (columns)\nAdd Filters\nAdd and/or Delete Rows\n\n\n\n\nAnalyses tab\nIt includes the available statistical analyses that can be performed by Jamovi.\n\n\nWe will spend most of our time in the Analyses Tab. The following six modules are pre-installed:\n\nExploration\nT-Tests\nANOVA\nRegression\nFrequencies\nFactor\n\nFor example, if we want to perform regression analysis, we simply click the ’’Regression” button.\nAll other modules need to be installed using the Modules button (Plus button) in our top-right  |\n\n\n\nEdit tab\nIt includes a toolbar similar to a word processor.\n\n\nWe can add extra information to our results using the buttons that are very similar to what we would find in Word (though there are fewer options).\n\n\n\nSettings tab\n(the three dots \\({\\vdots}\\) at the top right of the window)\n\nIt includes the application settings that can be manged by the users according to their preferences.\n\n\nWe can apply our preferences for a number of settings such as:\n\nHow many decimal numbers we want.\nIf we want to learn R, we can also display the R syntax.\nOur graph colour scheme\nOur default missing value.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#types-of-variables-in-jamovi",
    "href": "lab1.html#types-of-variables-in-jamovi",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "3.4 Types of Variables in Jamovi",
    "text": "3.4 Types of Variables in Jamovi\nData variables can be one of four measure types:\n\n Nominal: This type is for nominal categorical variables.\n Ordinal: This type is for ordinal categorical variables.\n Continuous : this type is for variables with numeric values which are considered to be of Interval or Ratio scales.\n ID: This will usally be our first column. This can be text or numbers, but it should be unique to each row.\n\n \nAdditionally, data variables can be one of three data types:\n\nInteger: These are full numbers e.g. 1, 2, 3, … 100, etc. - Integers can be used for all three measure types . When used for Nominal/Ordinal data numbers will represent labels e.g. male=1; female=2.\nDecimal: These are numbers with decimal points. e.g. 1.3, 5.6, 7.8, etc. - This will usually only be used for continuous data.\nText: This can be used for ordinal and nominal data.\n\nThe measure types are designated by the symbol in the header of the variable’s column. Note that some combinations of data-type and measure-type don’t make sense, and Jamovi won’t let us choose these.\n\n\n\nTable 3.2: Types of data and measures\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\n\n\nData\nNominal\nOrdinal\nContinuous\n\n\nInteger\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)\n\n\nDecimal\n\n\n\\({\\checkmark}\\)\n\n\nText\n\\({\\checkmark}\\)\n\\({\\checkmark}\\)",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#importing-data",
    "href": "lab1.html#importing-data",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "3.5 Importing data",
    "text": "3.5 Importing data\n\n3.5.1 The dataset\nIt is possible to simply begin typing values into the Jamovi spreadsheet as we would with any other spreadsheet software. Alternatively, existing datasets in a range of formats (OMV, Excel, CSV, SPSS, R data, Stata, SAS) can be opened in Jamovi. We will use the following dataset as an example (Figure 3.4).\n\n\n\n\n\n\n\n\nFigure 3.4: Table with raw data.\n\n\n\n\nThe meta-data (data about the data) for this dataset are as following:\n\nsex: sex (1 = male, 2 = female)\nage: age in years\ntime_spend: hours spent on social media\n\\(q_1 ... q_{10}\\): Ten questions (items) of Rosenberg Self-Esteem Scale (RSES). The 10 items are answered on a four point scale ranging from strongly disagree to strongly agree.(0 = Strongly Disagree, 1 = Disagree, 2 = Agree, 3 = Strongly Agree)\n\n\n\n3.5.2 Opening the file\nTo open this csv file, click on the File tab  at the top left hand corner (just left of the Variables tab) (Figure 3.5).\n\n\n\n\n\n\nFigure 3.5: Click on the File tab\n\n\n\nThis will open the menu shown in Figure 3.6. Select ‘Open’ and then ‘This PC’. Choose the downloaded file from the files listed on ‘Browse’ which are stored on our computer folders:\n\n\n\n\n\n\n\n\nFigure 3.6: Open an existing file stored on our computer into Jamovi.\n\n\n\n\n\n \nThe flowchart of the process is:\n\n\n\n\n\nflowchart LR\n  A(File tab) -.-&gt; B(Open) -.-&gt; C(This PC) -.-&gt; D(Browse) -.-&gt; E(Open \\n 'the downloaded file')\n\n\n\n\n\n\n \nWe should see data now in the Spreadsheet view (Figure 3.7).\n\n\n\n\n\n\n\n\nFigure 3.7: Our dataset.\n\n\n\n\n\nAs we can see this is a data set with 258 observations and 13 variables. JAMOVI has classified all the variables as nominal ; however, only the sex variable is actually nominal.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab1.html#transforming-data",
    "href": "lab1.html#transforming-data",
    "title": "3  LAB I: Introduction to Jamovi",
    "section": "3.6 Transforming data",
    "text": "3.6 Transforming data\n\n3.6.1 Changing the measure type\nFirst, we will change the measure type from nominal to continuous for all variables except sex. For example, double-click on the variable name age. This will open an additional menu at the top of the JAMOVI screen, as shown in Figure 3.8:\n\n\n\n\n\n\nFigure 3.8: Extra menu for data variables.\n\n\n\nFrom the drop-down list of “Measure type” we select the continuous type , as shown in Figure 3.9.\n\n\n\n\n\n\nFigure 3.9: Extra menu for data variables.\n\n\n\nThe categorical variable sex is coded as 1 for males and 2 for females, and it is stored as nominal .",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LAB I: Introduction to Jamovi</span>"
    ]
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "4  LAB IV: Sampling distribution and Confidence Interval",
    "section": "",
    "text": "4.1 The Sampling Distribution of mean and the CLT\nIn this Lab we will learn the Central Limit Theorem (CLT), which is the basis for many statistical concepts. We are going to explore this concept with the help of a Shiny application. So, clink on the following link CLM.\nA Shiny app opens in a web window as shown below (Figure 4.1):\nTo the left is the interactive panel with radio buttons and slider bars, and to the right there are three tabs:\nFirst we are asked to choose from a Normal, Uniform, Right Skewed or Left Skewed Parent distribution (Population) from the left panel. Let’s select Right skewed and then High skew from the drop down menu with the name Skew, as shown in Figure 4.2.\nNext we set the Sample size slider bar to 5 and the Number of samples to 1000, then select the Samples tab, as shown in Figure 4.3. The first eight samples randomly drawn from the original distribution are demonstrated in the panel. For example, in the first box labeled Sample 1, we observe five data points (the sample size we set), along with their sample mean and standard deviation (highlighted in the red circle).\nFinally, we select the Sampling distribution tab, which displays the distribution of the 1000 sample means. We observe that this distribution is right skewed with mean approximately equal to population mean (Figure 4.4).\nNow, try to increase the sample size to 30 (Figure 4.5):\nand then increase it to 200 (Figure 4.6):\nWe observe that the sampling distribution becomes closer and closer to Normal and the standard error of the mean, SE, (the standard deviation of sample means) gets smaller as the sample size increases. The important point is that whatever the parent distribution of a variable, the distribution of the sample means will be nearly Normal, as long as the samples are large enough.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LAB IV: Sampling distribution and Confidence Interval</span>"
    ]
  },
  {
    "objectID": "lab4.html#the-sampling-distribution-of-mean-and-the-clt",
    "href": "lab4.html#the-sampling-distribution-of-mean-and-the-clt",
    "title": "4  LAB IV: Sampling distribution and Confidence Interval",
    "section": "",
    "text": "Figure 4.1: The Shiny application simulating the Central limit Theorem for Means\n\n\n\n\n\nPopulation Distribution.\nSamples.\nSampling Distribution.\n\n\n\n\n\n\n\n\nFigure 4.2: The case of a high right skewed population distribution\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: First eight samples randomly drawn from the original distribution.\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Distribution of means of 1000 random samples, each consisting of 5 observations from a high right skewed population distribution\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Distribution of means of 1000 random samples, each consisting of 30 observations from a high right skewed population distribution\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Distribution of means of 1000 random samples, each consisting of 200 observations from a high right skewed population distribution\n\n\n\n\n\n\n\n\n\n\n\nProperties of the sampling distribution of the mean\n\n\n\n\nThe mean of the sampling distribution is the same as the mean of the population.\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases.\nAccording to the Central Limit Theorem (CLM), the shape of the sampling distribution becomes normal as the sample size increases regardless of the variable’s population distribution.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LAB IV: Sampling distribution and Confidence Interval</span>"
    ]
  },
  {
    "objectID": "lab4.html#the-confidence-interval-of-mean",
    "href": "lab4.html#the-confidence-interval-of-mean",
    "title": "4  LAB IV: Sampling distribution and Confidence Interval",
    "section": "4.2 The confidence interval of mean",
    "text": "4.2 The confidence interval of mean\nWe are going to explore the concept of confidence interval (CI) of mean with the help of a Shiny application. So, clink on the following link CIs.\nA Shiny app opens in a web window as shown below (Figure 4.7):\n\n\n\n\n\n\nFigure 4.7: Shiny application that simulates the concept of confidence interval (CI) of mean\n\n\n\nOn the left is the interactive panel with radio buttons and drop down menus, and to the right there are two tabs:\n\nPlots.\nAbout.\n\nWe retain active the Plots tab.\nFirst we are asked to choose if we want the Confidence Interval Graph only or the Confidence Interval Graph Plus Sampling Distribution of the Mean. Let’s select the first choice and set the Number of Simulated Samples to one and the Sample Size to 10 from the drop down menus, as shown in Figure 4.8. A horizontal bar will be created which represents the confidence interval (CI), centered on the sample mean (point). In this case, the 95% CI for the sample mean includes the true value of the population mean (it crosses the solid vertical line) and it is drawn as a black line.\n\n\n\n\n\n\nFigure 4.8: Confidence Interval Graph with one random sample of 10 observations selected from a normal population distribution\n\n\n\nNow, try to increase the Number of Simulated Samples to 100 (Figure 4.9):\n\n\n\n\n\n\nFigure 4.9: Confidence Interval Graph with 100 random samples, each consisting of 10 observations from a normal population distribution\n\n\n\nWe observe that 5 out of 100 confidence intervals (red horizontal lines) do not include the true population mean (the solid vertical line) (Figure 4.9). This is what we would expect – that the 95% confidence interval will not include the true population mean 5% of the time.\n \nNext, we create the confidence intervals of 100 randomly generated samples of size = 50 from the population (Figure 4.10):\n\n\n\n\n\n\nFigure 4.10: Confidence Interval Graph with 100 random samples, each consisting of 50 observations from a normal population distribution\n\n\n\nWe observe that the sample means are closer to the true population mean and the 95% CIs of the mean become narrower (Figure 4.10) increasing the sample size.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LAB IV: Sampling distribution and Confidence Interval</span>"
    ]
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "5  LAB VI: Inference for numerical data (2 samples)",
    "section": "",
    "text": "5.1 Two-sample t-test (Student’s t-test)\nTwo sample t-test (Student’s t-test) can be used if we have two independent (unrelated) groups (e.g., males-females, treatment-non treatment) and one quantitative variable of interest.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LAB VI: Inference for numerical data (2 samples)</span>"
    ]
  },
  {
    "objectID": "lab6.html#two-sample-t-test-students-t-test",
    "href": "lab6.html#two-sample-t-test-students-t-test",
    "title": "5  LAB VI: Inference for numerical data (2 samples)",
    "section": "",
    "text": "5.1.1 Opening the file\nOpen the dataset named depression from the file tab in the menu:\n\n\n\n\n\n\nFigure 5.1: The depression dataset\n\n\n\nThe dataset depression includes 76 patients and has two variables. The treatment variable and the HDRS variable (Figure 5.1). Double-click on the variable name HDRS and change the measure type from nominal  to continuous .\n\n\n5.1.2 Research question\nIn an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (intervention Vs placebo).\nThe researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level α was set to 0.05.\nNote A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\n\n\n5.1.3 Hypothesis Testsing for the Student’s t-test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))\n\n\n\n\n\n5.1.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the descriptive characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) and for equality of variances (e.g., Levene’s test) can be used.\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Exploration) -.-&gt; C(Descriptives)\n\n\n\n\n\n\nas shown below in Figure 5.2.\n\n\n\n\n\n\nFigure 5.2: In the Analyses Tab select Exploration and click on Descriptives.\n\n\n\nThe Descriptives dialogue box opens. Drag the variable HDRS into the Variables box and split it by the treatment variable, as shown below (Figure 5.3):\n\n\n\n\n\n\nFigure 5.3: Split the variable HDRS by treatment group\n\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section:\n\n\n\n\n\n\nFigure 5.4: In the Statistics section select the descriptive statistics of interest.\n\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below:\n\n\n\n\n\n\nFigure 5.5: Descriptive statistics of HDSR by treatment group\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis is close to zero (mesokurtic distribution) indicating normal distributions for both groups.\nAdditionally, the Shapiro-Wilk tests of normality suggest that the data for the HDRS in both groups, paroxetine and placebo, are normally distributed (p=0.67 &gt;0.05 and p=0.61 &gt;0.05, respectively). (NOTE: If the \\(p \\geq 0.05\\), then the data came from a normally distributed population).\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThen we can check the Density from Histograms in the Plot section, as shown below (Figure 5.7):\n\n\n\n\n\n\nFigure 5.6: In the Plot section select Density from Histograms.\n\n\n\nA graph is generated in the output window on our right-hand side, as shown below:\n\n\n\n\n\n\nFigure 5.7: In the Plots section select Density from Histograms.\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\nB. Homogeneity of variance\nThe second assumption that should be satisfied is the homogeneity of variance. We observe in the summary table of Figure 5.5 that the two standard deviations (3.65 vs 3.41) are similar (see also below the Levene’s test for equality of variances in Figure 5.11).\n \n\n\n5.1.5 Run the Student’s t-test\n\n\n\n\n\n\nPerform a Student’s t-test\n\n\n\nWe will perform a Student’s t-test to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(T-Tests) -.-&gt; C(Independent Samples T-Test)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.8: Conducting an Independent Samples T-Test.\n\n\n\nThe Independent Samples T-Test dialogue box opens. Drag and drop the numeric variable HSDR to Dependent Variables and the independent variable treatment to Grouping Variable, as shown below Figure 5.9:\n\n\n\n\n\n\nFigure 5.9: The Independent Samples T-Test dialogue box\n\n\n\nWe observe that we can select between the following three Tests: Students’s (the default), Welch’s, or Mann-Whitney U. At the moment, we keep the default choice of Students’s test. From Additional Statistics check the Mean difference, Confidence Intervals, Descriptive, and Descriptive plots boxes. Finally, from Assumption Checks tick the Homogeneity test. We will end up with the following screen:\n\n\n\n\n\n\nFigure 5.10: Additional statistics and tests.\n\n\n\nFirst, we look at the table of Levene's test for equality of variances (Figure 5.11):\n\n\n\n\n\n\nFigure 5.11: Levene’s test.\n\n\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of HDRs in two groups are equal\n\\(H_{1}\\): the variances of HDRs in two groups are not equal\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nSince p = 0.646 &gt; 0.05, the \\(H_0\\) of the Levene’s test is not rejected and we keep the default choice of Students’s test (Figure 5.10). (NOTE: If the \\(p \\geq 0.05\\), then the population variances of HDRS in two groups groups are assumed equal).\nIf the assumption of equal variances is not satisfied (Levene’s test gives p &lt; 0.05, reject \\(H_0\\)), the Welch’s test should be used from the available Tests in Jamovi (Figure 5.10).\nNext, we can inspect again the results in the group descriptives table (Figure 5.12) and pertinent plots (Figure 5.13):\n\n\n\n\n\n\nFigure 5.12: Group descriptives.\n\n\n\n\n\n\n\n\n\nFigure 5.13: Plot of mean (95% CI) and median of HDRS by treatment.\n\n\n\nFinally, we present the results of the Student’s t-test in the table of the Figure 5.14:\n\n\n\n\n\n\nFigure 5.14: The results of the Student’s t-test.\n\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine sample (n= 33) has 32 (33-1) degrees of freedom and the placebo sample (n= 43) has 42 (43-1), so we have 74 (32 + 42) df in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have 76-2 = 74 df.\nThe Student t-test for two independent samples does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because when a total of 2n subjects are available, their equal division among the groups maximizes the power to detect a specified difference.\n\n\n\n\n\n\n\n\nMann-Whitney U test\n\n\n\nWhen there is violation of normality, the Mann-Whitney U test can be selected from the available Tests in Jamovi (Figure 5.11). This test compares two independent samples based on the ranks of the values and is often considered the non-parametric equivalent to the Student’s t-test.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LAB VI: Inference for numerical data (2 samples)</span>"
    ]
  },
  {
    "objectID": "lab6.html#paired-samples-t-test",
    "href": "lab6.html#paired-samples-t-test",
    "title": "5  LAB VI: Inference for numerical data (2 samples)",
    "section": "5.2 Paired samples t-test",
    "text": "5.2 Paired samples t-test\nThe paired samples design can effectively reduce the effect of non-treatment factors and improve the efficiency of the experiment. A paired samples t-test is used to estimate whether the means of two related measurements are significantly different from one another.\nOpen the dataset named weight from the file tab in the menu:\n\n\n\n\n\n\nFigure 5.15: The weight dataset\n\n\n\nThe dataset weight contains the birth and discharge weight of 25 newborns (Figure 5.15). Double-click on the name of the variables birth_weight and discharge_weight to change the measure type from nominal  to continuous .\n\n5.2.1 Research question\nWe might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\n\n\n5.2.2 Hypothesis Testsing for the paired samples t-test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\nH0: the mean difference in weight is zero (\\(\\mu_{d} = 0\\))\nH1: the mean difference in weight is non-zero (\\(\\mu_{d} \\neq 0\\))\n\n\n\n\n\n5.2.3 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\n\nThe differences between the pairs of measurements, \\(d_{i}\\)s, are normally distributed. (NOTE: It is not essential for the original observations to be normally distributed).\n\n\n\nExplore the characteristics of the distribution of differences, \\(d_{i}\\)\nFirst, we have to calculate the differences \\(d_{i}= birth\\_weight_i - discharge\\_weight_i\\) (Figure 5.16) from Data tab in the main menu of Jamovi. For more details go to the section 11.6 Transforming data: Computing a new variable in Chapter 3.\n\n\n\n\n\n\nFigure 5.16: Calculation of the variable of differences d\n\n\n\nThe distributions of the differences,\\(d_{i}\\), can be explored with appropriate plots and summary statistics.\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Exploration) -.-&gt; C(Descriptives)\n\n\n\n\n\n\nas shown below in Figure 5.17.\n\n\n\n\n\n\nFigure 5.17: In the Analyses Tab select Exploration and click on Descriptives.\n\n\n\nThe Descriptives dialogue box opens. Drag the variable d into the Variables box, as shown below (Figure 5.18):\n\n\n\n\n\n\nFigure 5.18: Drag the variable of the differences d into the Variables box\n\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section:\n\n\n\nIn the Statistics section select the descriptive statistics of interest.\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below:\n\n\n\n\n\n\nFigure 5.19: Descriptive statistics of the differences.\n\n\n\nThe mean is close to median (39.6 vs 40). Moreover, both skewness and (excess) kurtosis are approximately zero indicating a symmetric and mesokurtic distribution of the weight differences.\nThen we can check the Density from Histograms in the Plot section, as shown below (Figure 6.6):\n\n\n\nIn the Plot section select Density from Histograms.\n\n\nA graph is generated in the output window on our right-hand side, as shown below:\n\n\n\n\n\n\nFigure 5.20: In the Plots section select Density from Histograms.\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\nAdditionally, the Shapiro-Wilk test of normality suggests that the data for the differences, \\(d_{i}\\), are normally distributed (p=0.74 &gt;0.05). (NOTE: If the \\(p \\geq 0.05\\), then the data came from a normally distributed population).\n\n\n5.2.4 Run the paired samples t-test\n\n\n\n\n\n\nPerform a paired samples t-test\n\n\n\nWe will perform a paired samples t-test to test the null hypothesis that the mean difference in weight is zero.\nWe select:\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(T-Tests) -.-&gt; C(Paired Samples T-Test)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.21: Conducting a Paired Samples T-Test.\n\n\n\nThe Paired Samples T-Test dialogue box opens. Drag and drop the variables birth_weight and discharge_weight to Paired Variables, as shown below Figure 5.22:\n\n\n\n\n\n\nFigure 5.22: The Paired Samples T-Test dialogue box\n\n\n\nWe observe that we can select between the following two Tests: Students’s or Wilcoxon rank. We keep the default choice of Students’s paired t-test. Moreover, from Additional Statistics check the Mean difference, Confidence Intervals, Descriptive, and Descriptive plots boxes. Finally, from Assumption Checks tick the Normality test. We will end up with the following screen:\n\n\n\n\n\n\nFigure 5.23: Additional statistics and tests.\n\n\n\nNext, we can inspect the results in the table with descriptive statistics (Figure 5.24) and plots (Figure 5.13):\n\n\n\n\n\n\nFigure 5.24: Table with descriptive statistics.\n\n\n\n\n\n\n\n\n\nFigure 5.25: Plot of mean and median of birth weigt and discharge_weight.\n\n\n\nThe Shapiro-Wilk test of normality of the differences has previously calculated (Figure 5.19) and is also presented below:\n\n\n\n\n\n\nFigure 5.26: Test of normality of the differences.\n\n\n\nFinally, we present the results of the Student’s paired samples t-test in the table of the Figure 5.27:\n\n\n\n\n\n\nFigure 5.27: The results of the Paired Samples t-test.\n\n\n\nThere was a significant reduction in weight (39.6 g) after the discharge (p-value &lt;0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 52.9) doesn’t include the null hypothesized value of 0. However, is this reduction of clinical importance?\n\n\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\nWhen there is violation of normality, the Wilcoxon Rank test can be selected from the available Tests in Jamovi (Figure 5.23). This test is based on the sign and the magnitude of the rank of the differences between pairs of measurements, rather than the actual values. It is often considered the non-parametric equivalent to the Student’s paired samples t-test.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LAB VI: Inference for numerical data (2 samples)</span>"
    ]
  },
  {
    "objectID": "lab7.html",
    "href": "lab7.html",
    "title": "6  LAB VII: Inference for numerical data (>2 samples)",
    "section": "",
    "text": "6.1 Introduction\nThe one-way analysis of variance (one-way ANOVA) or the non-parametric Kruskal-Wallis test are used to detect whether there are any differences between more than two independent (unrelated) samples.\nAlthough, these tests can detect a difference between several groups they do not inform about which groups are different from the others. At first sight we might clarify the question by comparing all groups in pairs with t-tests or Mann-Whitney U tests. However, that procedure may lead us to the wrong conclusions (known as multiple comparisons problem).\nWhy is this procedure inappropriate? Quite simply, because we would be wrongly testing the null hypothesis. Each comparison one conducts increases the likelihood of committing at least one Type I error within a set of comparisons (famillywise Type I error rate).\nThis is the reason why, after an ANOVA or Kruskal-Wallis test concluding on a difference between groups, we should not just compare all possible pairs of groups with t-tests or Mann-Whitney U tests. Instead we perform statistical tests that take into account the number of comparisons (post hoc tests). Some of the more commonly used ones are Tukey test, Games-Howell test, and Bonferroni correction.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LAB VII: Inference for numerical data (\\>2 samples)</span>"
    ]
  },
  {
    "objectID": "lab7.html#one-way-analysis-of-variance-anova",
    "href": "lab7.html#one-way-analysis-of-variance-anova",
    "title": "6  LAB VII: Inference for numerical data (>2 samples)",
    "section": "6.2 One-way Analysis of Variance (ANOVA)",
    "text": "6.2 One-way Analysis of Variance (ANOVA)\nOne-way analysis of variance, usually referred to as one-way ANOVA, is a statistical test used when we want to compare several means. We may think of it as an extension of Student’s t-test to the case of more than two samples.\n\n6.2.1 Opening the file\nOpen the dataset named “dataDWL” from the file tab in the menu:\n\n\n\n\n\n\nFigure 6.1: The dataDWL dataset\n\n\n\nThe dataset “dataDWL” has 60 participants and includes two variables (Figure 8.4). The numeric WeightLoss variable and the Diet variable (with levels A, B, C and D).\n\n\n6.2.2 Research question\nConsider the example of the variations between weight loss according to four different types of diet (A, B, C, and D). The question that may be asked is: does the average weight loss (units in kg) differ according to the diet?\n\n\n6.2.3 Hypothesis Testsing for the ANOVA test\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal: \\(\\mu_{A} = \\mu_{B} = \\mu_{C} = \\mu_{D}\\))\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)\n\n\n\n\n\n6.2.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe dependent variable, WeightLoss, should be approximately normally distributed for all groups\nThe data in groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the descriptive characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) and for equality of variances (e.g., Levene’s test) can be used.\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Exploration) -.-&gt; C(Descriptives)\n\n\n\n\n\n\nas shown below in Figure 6.2.\n\n\n\n\n\n\nFigure 6.2: In the menu at the top, choose Analyses &gt; Exploration  &gt; Descriptives.\n\n\n\nThe Descriptives dialogue box opens. Drag the variable WeightLoss into the Variables field and split it by the Diet variable. Additionally, select Variable across rows, as shown below (Figure 6.3):\n\n\n\n\n\n\nFigure 6.3: Split the variable WeightLoss by Diet group and select Variables across rows.\n\n\n\nWe can now select the relevant descriptive statistics such as Percantiles, Skewness, Kurtosis and the Shapiro-Wilk test from the Statistics section (Figure 6.4):\n\n\n\n\n\n\nFigure 6.4: In the Statistics section select the descriptive statistics of interest.\n\n\n\nOnce we have selected our descriptive statistics, a table will appear in the output window on our right-hand side, as shown below (Figure 6.5):\n\n\n\n\n\n\nFigure 6.5: Descriptive statistics of WeightLoss by Diet group (click on figure to zoom in).\n\n\n\nThe means are close to medians and the standard deviations are also similar indicating normal distributions for all groups. Additionally, both shape measures, skewness and (excess) kurtosis, have values in the acceptable range [-1, 1] which indicate symmetric and mesokurtic distributions, respectively.\nThe Shapiro-Wilk tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p &gt; 0.05 \\(\\Rightarrow H_0\\) is not rejected).\n\n\n\n\n\n\nRemember: Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population.\n\\(H_{1}\\): the data tested are not normally distributed.\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nThen we can check the Density box from Histograms in the Plot section, as shown below (Figure 6.6):\n\n\n\n\n\n\nFigure 6.6: In the Plots section select Density from Histograms.\n\n\n\nA graph (Figure 6.7) is generated in the output window on our right-hand side:\n\n\n\n\n\n\nFigure 6.7: The Density plot of WeightLoss for each Diet.\n\n\n\nThe above density plots show that the data are close to symmetry and the assumption of a normal distribution is reasonable for all diet groups.\nB. Homogeneity of variance\nThe second assumption that should be satisfied is the homogeneity of variance. We observe in the summary table of Figure 6.5 that the standard deviations are similar (see also below the Levene’s test for equality of variances in Figure 5.11).\n \n\n\n6.2.5 Run the ANOVA test\n\n\n\n\n\n\nPerform ANOVA in Jamovi\n\n\n\nWe will perform ANOVA to test the null hypothesis that the mean WeightLoss is the same for all Diet groups.\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(ANOVA) -.-&gt; C(One-Way ANOVA)\n\n\n\n\n\n\nas shown below in Figure 6.8.\n\n\n\n\n\n\nFigure 6.8: Conducting ANOVA test in Jamovi. In the menu at the top, choose Analyses &gt; ANOVA  &gt; One-Way ANOVA.\n\n\n\nThe One-Way ANOVA dialogue box opens. Drag and drop the WeightLoss to Dependent Variables field and the Diet to Grouping Variable, as shown below Figure 6.9:\n\n\n\n\n\n\nFigure 6.9: One-Way ANOVA dialogue box. Drag the WeightLoss into the Dependent Variables field and the Diet into the Grouping Variable field.\n\n\n\nWe observe that we can select between the following two Tests: Welch’s test (the default), or Fisher’s test. At the moment, we keep the default choice. Moreover, from Additional Statistics check the Descriptive and Descriptive plots boxes. Finally, from Assumption Checks tick the Homogeneity test box. We will end up with the following screen:\n\n\n\n\n\n\nFigure 6.10: Additional statistics and tests.\n\n\n\n\n\nFirst, we look at the table of Levene’s test for equality of variances (Figure 6.11):\n\n\n\n\n\n\nFigure 6.11: Levene’s test.\n\n\n\n\n\n\n\n\n\nRemember: Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\\(H_{0}\\): the variances of WeightLoss in all groups are equal (\\(σ^2_A=σ^2_B=σ^2_C=σ^2_D\\))\n\\(H_{1}\\): the variances of WeightLoss differ between groups (\\(σ^2_i\\neq σ^2_j\\), where \\(i,j= A, B, C, D\\) and \\(i\\neq j\\))\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n\nSince p = 0.583 &gt; 0.05, the \\(H_0\\) of the Levene’s test is not rejected and we have to perform the Fisher’s test which assumes equal variances (Figure 6.12). So, let’s tick on the Assume equal (Fisher’s) box. (NOTE: If the \\(p \\geq 0.05\\), then the population variances of WeightLoss in all groups are assumed equal).\n\n\n\n\n\n\nFigure 6.12: Fisher’s ANOVA test.\n\n\n\n\n\n\n\n\n\nANOVA test (Welch’s option)\n\n\n\nIf the assumption of equal variances is not satisfied (Levene’s test gives p &lt; 0.05, reject \\(H_0\\)), the Welch’s option of ANOVA should be used from the available Tests in Jamovi (Figure 6.12).\n\n\nNext, we can inspect again the results in the group descriptives table (Figure 6.13) and pertinent plots (Figure 6.13):\n\n\n\n\n\n\nFigure 6.13: Group descriptive statistics.\n\n\n\n\n\n\n\n\n\nFigure 6.14: Plot of mean (95% CI) of WeightLoss by Diet.\n\n\n\nFrom the Figure 6.14 we observe that the participants following the diet C have on average the higher weight loss.\nFinally, we present the results of the Fisher’s ANOVA test in the table of Figure 6.15:\n\n\n\n\n\n\nFigure 6.15: The results of the Fisher’s ANOVA test.\n\n\n\nIn Figure 6.15, F= 6.12 indicates the F-statistic:\n\\[F= \\frac{variation \\ between \\ sample \\ means}{variation \\ within \\ the \\ samples}\\]\nNote that we compare this value to an F-distribution (F-test). The degrees of freedom in the numerator (df1) and the denominator (df2) are 3 and 56, respectively.\nThe p-value=0.001 is less than 0.05 (reject \\(H_0\\) of the ANOVA test). There is at least one diet with mean weight loss which is different from the other means.\n \n\n\n6.2.6 Run post-hoc tests\n\n\n\n\n\n\nPerform post-hoc tests\n\n\n\nA significant one-way ANOVA is generally followed by post-hoc tests to perform multiple pairwise comparisons between groups. From the One-Way ANOVA dialogue box click on Post-Hoc Tests section. We have got the following two options:\n\nGames-Howell (unequal variances)\nTukey (equal variances)\n\nBased on the result of Levene’s test (p = 0.583 &gt; 0.05, the \\(H_0\\) is not rejected) (Figure 6.11), we should select the Tukey (equal variances) post-hoc test. Additionally, check the Flag significant comparisons as shown below (Figure 6.16):\n\n\n\n\n\n\nFigure 6.16: Select the appropriate post-hoc test. For equal variances the Tukey post hoc test.\n\n\n\nOnce we have selected our post-hoc test, a table will appear in the output window on our right-hand side, as shown below (Figure 6.17):\n\n\n\n\n\n\nFigure 6.17: Table with the results of the Tukey post-hoc test.\n\n\n\nInterpretation\nPairwise comparisons were carried out using the method of Tukey and the adjusted p-values were calculated. The weight loss following diet C is significantly larger compared to diet A (mean difference = 2.93 kg, p=0.005 &lt;0.05) or diet B (mean difference = 3.21 kg, p=0.002 &lt;0.05).",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LAB VII: Inference for numerical data (\\>2 samples)</span>"
    ]
  },
  {
    "objectID": "lab10.html",
    "href": "lab10.html",
    "title": "7  LAB X: Simple linear regression",
    "section": "",
    "text": "When we have finished this Lab, we should be able to:\n\n\n\n\n\n\nLearning objectives\n\n\n\n\nUnderstand the linear regression model\nExplore how a factor (independent variable) affect a response (dependent) variable.\nInterpret the results\n\n\n\nIn this Lab, we will use the “LungCapacity” dataset.\n\n7.0.1 Opening the file\nOpen the dataset named “LungCapacity” from the file tab in the menu:\n\n\n\n\n\n\nFigure 7.1: The LungCapacity dataset\n\n\n\nDouble-click on the variable name Age and change the measure type from nominal  to continuous .\n\n\n7.0.2 Research question\nLet’s say that we want to model the association between age (in years) and lung capacity (in liters) for the sample of 725 participants in a survey. In other words, we want to find the parameters of a mathematical equation such as \\(y = \\alpha + \\beta \\cdot x\\).\n\n\n7.0.3 Hypothesis Testsing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_{0}:\\) the two variables are not linearly related. There is no effect between age and lung capacity (\\(β = 0\\)).\n\\(H_{1}:\\) the two variables are linearly related. There is an effect between age and lung capacity (\\(β \\neq 0\\)).\n\n\n\n\n\n7.0.4 Scatter plot\nWe start our analysis by creating the scatter plot of the response variable LungCap and the explanatory variable Age.\n\n\n\n\n\n\nFigure 7.2: The Scatter plot of Age and Lung Capacity\n\n\n\nThere is a clear upward trend indicating that increase in Age tends to coincide with increase in LungCap. Moreover, the trend seems to be linear, so a straight line can capture the overall pattern.\n\n\n7.0.5 Linear regression\nThe process of fitting a linear regression model to the data involves finding a straight line that can be considered as the best representation of the overall association between age and lung capacity.\nTo choose a line, we need to explain what we mean by the “best representation” of the data. A “best-fitting” line refers to the line that minimizes the sum of squared residuals (RSS). Therefore, we refer to the resulting model as the least-squares linear regression model and to the corresponding line as the least-squares regression line.\n\n\n7.0.6 Fit a simple linear regression model\nOn the Jamovi top menu navigate to\n\n\n\n\n\nflowchart LR\n  A(Analyses) -.-&gt; B(Regression) -.-&gt; C(Linear Regression)\n\n\n\n\n\n\nas shown below (Figure 7.3).\n\n\n\n\n\n\nFigure 7.3: In the menu at the top, choose Analyses &gt; Regression &gt; Linear Regression.\n\n\n\nThe Linear Regression dialogue box opens (Figure 7.4). From the left-hand pane drag the variable LunCap into the Dependent Variable field and the variable Age into the Covariates field on the right-hand side, as shown below:\n\n\n\n\n\n\nFigure 7.4: The Linear Regression dialogue box options. Drag and drop the LunCap into the Dependent Variable field and the Age into the Covariates field.\n\n\n\nAdditionally, from the Model Coefficients section tick the box “Confidence interval” in Estimate (Figure 7.5):\n\n\n\n\n\n\nFigure 7.5: Check the Confidence interval box in the Model Coefficients section.\n\n\n\nThe output table with the model coefficients should look like the following (Figure 7.6):\n\n\n\n\n\n\nFigure 7.6: The model coefficients table.\n\n\n\nNow, let’s find the model equation from the regression table in Figure 7.6. In the Estimate column are the intercept \\(a=0.54\\) and the slope \\(b=0.26\\) for Age. Thus, the equation of the regression line becomes:\n\\[\n\\begin{aligned}\n\\widehat{y} &= a + b \\cdot x\\\\\n\\widehat{\\text{LungCap}} &= a + b \\cdot\\text{Age}\\\\\n\\widehat{\\text{LungCap}}&= 0.54 + 0.26\\cdot\\text{Age}\n\\end{aligned}\n\\]\nFinally, the quality of our simple linear model is presented in Figure 7.7:\n\n\n\n\n\n\nFigure 7.7: The coefficient of determination \\(R^2\\).\n\n\n\nIn our example takes the value 0.67. It indicates that about 67% of the variation in lung capacity can be explained by the variation of the age. In simple linear regression \\(\\sqrt{0.67} = 0.82\\) which equals to the Pearson’s correlation coefficient, r.\n \n\n\n\n\n\n\nInterpretation of the results\n\n\n\nThe regression coefficient (b=0.26) of the Age is significantly different from zero (p &lt; 0.001) and indicates that there’s on average an increase of 0.26 liters in lung capacity for every 1 year increase in age. Note that the \\(95\\%\\)CI (0.24 to 0.27) does not include the hypothesized null value of zero for the slope.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LAB X: Simple linear regression</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "8  Data",
    "section": "",
    "text": "Dataset: creatinine\n\n\n\n\n\n\n\n\n\nFigure 8.1: Table with raw data.\n\n\n\n\n   \n\nDataset: depression\n\n\n\n\n\n\n\n\n\nFigure 8.2: Table with raw data.\n\n\n\n\n   \n\nDataset: weight\n\n\n\n\n\n\n\n\n\nFigure 8.3: Table with raw data.\n\n\n\n\n   \n\nDataset: dataDWL\n\n\n\n\n\n\n\n\n\nFigure 8.4: Table with raw data.\n\n\n\n\n   \n\nDataset: prematurity\n\n\n\n\n\n\n\n\n\nFigure 8.5: Table with raw data.\n\n\n\n\n   \n\nDataset: LungCapacity\n\n\n\n\n\n\n\n\n\nFigure 8.6: Table with raw data.",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "9  Presentations",
    "section": "",
    "text": "Yli 2024\n\n\n\n\nYli-psy-300\n\n\n     \n\nLecture 1: Introduction\n\n\n\n\npsy-300-1_introduction\n\n\n     \n\nLecture 2: Descriptive Statistics\n\n\n\n\npsy-300-2_descriptive\n\n\n     \n\nLecture 3: Probability Distributions\n\n\n\n\npsy-300-3_distributions\n\n\n     \n\nLecture 4: Sampling Distribution and Confidence Interval\n\n\n\n\npsy-300-4_sampling_distribution\n\n\n     \n\nLecture 5: Hypothesis testing\n\n\n\n\npsy-300-5_hypothesis_testing\n\n\n     \n\nLecture 6: Hypothesis testing for two samples\n\n\n\n\npsy-300-6_two_samples\n\n\n     \n\nLecture 7: ANOVA\n\n\n\n\npsy-300-7_ANOVA\n\n\n     \n\nLecture 8: Chi-square test\n\n\n\n\npsy-300-8_chi_square\n\n\n     \n\nLecture 9-10: Correlation and Simple Linear Regression\n\n\n\n\npsy-300-9_10_Correlation_linear_regression",
    "crumbs": [
      "Jamovi LAB",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Presentations</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Friligkou, Eleni, Solveig Løkhammer, Brenda Cabrera-Mendoza, Jie Shen,\nJun He, Giovanni Deiana, Mihaela Diana Zanoaga, et al. 2024. “Gene\nDiscovery and Biological Insights into Anxiety Disorders from a\nLarge-Scale Multi-Ancestry Genome-Wide Association Study.”\nNature Genetics, September. https://doi.org/10.1038/s41588-024-01908-2.\n\n\nGarcía, Jorge Acosta, Francisco Checa y Olmos, Manuel Lucas Matheu, and\nTesifón Parrón Carreño. 2019. “Self Esteem Levels Vs Global Scores\non the Rosenberg Self-Esteem Scale.” Heliyon 5 (3):\ne01378. https://doi.org/10.1016/j.heliyon.2019.e01378.\n\n\nGrimes, David A, and Kenneth F Schulz. 2002. “Bias and Causal\nAssociations in Observational Research.” The Lancet 359\n(9302): 248–52. https://doi.org/10.1016/s0140-6736(02)07451-2.\n\n\nHyndman, Rob J., and Yanan Fan. 1996. “Sample Quantiles in\nStatistical Packages.” The American Statistician 50 (4):\n361–65. https://doi.org/10.1080/00031305.1996.10473566.",
    "crumbs": [
      "References"
    ]
  }
]